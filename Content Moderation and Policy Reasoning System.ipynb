{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YTAvufu-nFwE"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain transformers accelerate sentencepiece\n",
        "!pip install -q langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio transformers torch accelerate sentencepiece"
      ],
      "metadata": {
        "id": "aTDJODKXAT-z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    pipeline\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Us5g6K1NnHm2",
        "outputId": "ca0e7ad7-6087-48d2-a498-ba9995856dc8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"Vrandan/Comment-Moderation\""
      ],
      "metadata": {
        "id": "0Q2ZUwWTna48"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "226I8kL8qArk",
        "outputId": "1cbd9c65-e715-461a-9c3a-51144e3e7c95"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
        ")\n",
        "\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWVHVzKsqLjj",
        "outputId": "9b9c9c28-5c88-47c3-c323-c0689c805106"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBertForSequenceClassification(\n",
              "  (distilbert): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x TransformerBlock(\n",
              "          (attention): DistilBertSdpaAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "moderation_pipeline = pipeline(\n",
        "    task=\"text-classification\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_all_scores=True,\n",
        "    device=0 if torch.cuda.is_available() else -1\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEChCoHwqPqT",
        "outputId": "a4eecae7-a041-43bf-f6c6-1bd2440c5233"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_text = \"I hope you have a great day!\"\n",
        "\n",
        "output = moderation_pipeline(test_text)\n",
        "output\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzJFUw4DqXir",
        "outputId": "5c3f8835-906d-4d7c-e1dc-f1a231cd6e11"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[{'label': 'S', 'score': 0.01796688884496689},\n",
              "  {'label': 'H', 'score': 0.01684916950762272},\n",
              "  {'label': 'V', 'score': 0.017463263124227524},\n",
              "  {'label': 'HR', 'score': 0.01866951957345009},\n",
              "  {'label': 'SH', 'score': 0.018865317106246948},\n",
              "  {'label': 'S3', 'score': 0.008961008861660957},\n",
              "  {'label': 'H2', 'score': 0.0071882931515574455},\n",
              "  {'label': 'V2', 'score': 0.010438548400998116},\n",
              "  {'label': 'OK', 'score': 0.9388498067855835}]]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_text = \"hate her, gonna beat her!\"\n",
        "\n",
        "output = moderation_pipeline(test_text)\n",
        "output\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEu2atfFsEiK",
        "outputId": "abd92988-529a-4414-e3fd-c35bbb67d537"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[{'label': 'S', 'score': 0.1803099662065506},\n",
              "  {'label': 'H', 'score': 0.7862440943717957},\n",
              "  {'label': 'V', 'score': 0.6674680113792419},\n",
              "  {'label': 'HR', 'score': 0.5422320365905762},\n",
              "  {'label': 'SH', 'score': 0.15856768190860748},\n",
              "  {'label': 'S3', 'score': 0.14313942193984985},\n",
              "  {'label': 'H2', 'score': 0.458370178937912},\n",
              "  {'label': 'V2', 'score': 0.17443899810314178},\n",
              "  {'label': 'OK', 'score': 0.047905780375003815}]]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEVERITY_LEVELS = {\n",
        "    \"LOW\": {\n",
        "        \"description\": \"Mild or contextual issues with no direct harm\"\n",
        "    },\n",
        "    \"MEDIUM\": {\n",
        "        \"description\": \"Potentially harmful content requiring restriction or warning\"\n",
        "    },\n",
        "    \"HIGH\": {\n",
        "        \"description\": \"Clear harmful or unsafe content requiring removal\"\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "Uc1ti8v2qa8r"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODERATION_POLICIES = {\n",
        "    \"HATE_SPEECH\": {\n",
        "        \"description\": \"Content that targets individuals or groups based on protected characteristics with abusive or demeaning language.\",\n",
        "        \"severity\": \"MEDIUM\",\n",
        "        \"examples\": [\n",
        "            \"insulting a protected group\",\n",
        "            \"calling for harm against a community\"\n",
        "        ]\n",
        "    },\n",
        "\n",
        "    \"HARASSMENT\": {\n",
        "        \"description\": \"Abusive or threatening language directed at an individual or group.\",\n",
        "        \"severity\": \"MEDIUM\",\n",
        "        \"examples\": [\n",
        "            \"personal insults\",\n",
        "            \"threatening tone without violence\"\n",
        "        ]\n",
        "    },\n",
        "\n",
        "    \"VIOLENCE\": {\n",
        "        \"description\": \"Content that promotes, glorifies, or threatens physical harm.\",\n",
        "        \"severity\": \"HIGH\",\n",
        "        \"examples\": [\n",
        "            \"threats of physical attack\",\n",
        "            \"encouraging violence\"\n",
        "        ]\n",
        "    },\n",
        "\n",
        "    \"SELF_HARM\": {\n",
        "        \"description\": \"Content that promotes or encourages self-harm or suicide.\",\n",
        "        \"severity\": \"HIGH\",\n",
        "        \"examples\": [\n",
        "            \"encouraging self-harm\",\n",
        "            \"suicide ideation\"\n",
        "        ]\n",
        "    },\n",
        "\n",
        "    \"SEXUAL_CONTENT\": {\n",
        "        \"description\": \"Explicit sexual content not suitable for general audiences.\",\n",
        "        \"severity\": \"HIGH\",\n",
        "        \"examples\": [\n",
        "            \"explicit sexual descriptions\"\n",
        "        ]\n",
        "    },\n",
        "\n",
        "    \"SAFE_CONTENT\": {\n",
        "        \"description\": \"Content that does not violate any policy.\",\n",
        "        \"severity\": \"LOW\",\n",
        "        \"examples\": [\n",
        "            \"neutral conversation\",\n",
        "            \"positive statements\"\n",
        "        ]\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "MyHxV6_cr7rj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DECISION_RULES = {\n",
        "    \"LOW\": \"ALLOW\",\n",
        "    \"MEDIUM\": \"RESTRICT\",\n",
        "    \"HIGH\": \"DISALLOW\"\n",
        "}"
      ],
      "metadata": {
        "id": "13OWkW7Hsi2L"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_reasoning_prompt(user_text, model_scores, policies):\n",
        "    \"\"\"\n",
        "    Constructs a policy-aware reasoning prompt.\n",
        "    \"\"\"\n",
        "\n",
        "    policy_descriptions = \"\\n\".join(\n",
        "        [f\"- {k}: {v['description']}\" for k, v in policies.items()]\n",
        "    )\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are a content moderation reasoning system.\n",
        "\n",
        "Your task:\n",
        "1. Analyze the user-generated content.\n",
        "2. Compare it strictly against the moderation policies below.\n",
        "3. Identify which policy (if any) is violated.\n",
        "4. Explain the reasoning using policy language only.\n",
        "5. If no policy is violated, clearly state that the content is safe.\n",
        "\n",
        "Moderation Policies:\n",
        "{policy_descriptions}\n",
        "\n",
        "Model Signals (for reference, not final decision):\n",
        "{model_scores}\n",
        "\n",
        "User Content:\n",
        "\\\"\\\"\\\"{user_text}\\\"\\\"\\\"\n",
        "\n",
        "Respond in a structured and concise manner.\n",
        "\"\"\"\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "aE7yBcVctRhS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_signals(text):\n",
        "    \"\"\"\n",
        "    Runs the Hugging Face moderation model\n",
        "    and returns label-score pairs.\n",
        "    \"\"\"\n",
        "    results = moderation_pipeline(text)[0]\n",
        "\n",
        "    signals = {item[\"label\"]: round(item[\"score\"], 4) for item in results}\n",
        "    return signals"
      ],
      "metadata": {
        "id": "3DfCgdPvuGla"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_text = \"I hate all of you and you deserve pain.\"\n",
        "\n",
        "model_signals = get_model_signals(user_text)\n",
        "\n",
        "reasoning_prompt = build_reasoning_prompt(\n",
        "    user_text=user_text,\n",
        "    model_scores=model_signals,\n",
        "    policies=MODERATION_POLICIES\n",
        ")\n",
        "\n",
        "print(reasoning_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11P__v6BuS1x",
        "outputId": "e1a4318e-34b2-4edb-c30d-1809bc6a8ed1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "You are a content moderation reasoning system.\n",
            "\n",
            "Your task:\n",
            "1. Analyze the user-generated content.\n",
            "2. Compare it strictly against the moderation policies below.\n",
            "3. Identify which policy (if any) is violated.\n",
            "4. Explain the reasoning using policy language only.\n",
            "5. If no policy is violated, clearly state that the content is safe.\n",
            "\n",
            "Moderation Policies:\n",
            "- HATE_SPEECH: Content that targets individuals or groups based on protected characteristics with abusive or demeaning language.\n",
            "- HARASSMENT: Abusive or threatening language directed at an individual or group.\n",
            "- VIOLENCE: Content that promotes, glorifies, or threatens physical harm.\n",
            "- SELF_HARM: Content that promotes or encourages self-harm or suicide.\n",
            "- SEXUAL_CONTENT: Explicit sexual content not suitable for general audiences.\n",
            "- SAFE_CONTENT: Content that does not violate any policy.\n",
            "\n",
            "Model Signals (for reference, not final decision):\n",
            "{'S': 0.047, 'H': 0.2959, 'V': 0.24, 'HR': 0.2156, 'SH': 0.198, 'S3': 0.032, 'H2': 0.0598, 'V2': 0.0481, 'OK': 0.1763}\n",
            "\n",
            "User Content:\n",
            "\"\"\"I hate all of you and you deserve pain.\"\"\"\n",
            "\n",
            "Respond in a structured and concise manner.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_policy_violation(model_signals, policies, threshold=0.1):\n",
        "    violated_policies = []\n",
        "\n",
        "    for model_label, score in model_signals.items():\n",
        "        if score >= threshold and model_label in MODEL_TO_POLICY_MAP:\n",
        "            policy_name = MODEL_TO_POLICY_MAP[model_label]\n",
        "\n",
        "            violated_policies.append({\n",
        "                \"policy\": policy_name,\n",
        "                \"severity\": policies[policy_name][\"severity\"],\n",
        "                \"confidence\": round(score, 3),\n",
        "                \"evidence_label\": model_label\n",
        "            })\n",
        "\n",
        "    return violated_policies"
      ],
      "metadata": {
        "id": "LCyc99Q3uWxy"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_moderation_decision(violations, decision_rules):\n",
        "    \"\"\"\n",
        "    Determines final moderation decision based on highest severity.\n",
        "    \"\"\"\n",
        "    if not violations:\n",
        "        return \"ALLOW\", \"No policy violations detected.\"\n",
        "\n",
        "    # Highest severity wins\n",
        "    severity_rank = {\"LOW\": 1, \"MEDIUM\": 2, \"HIGH\": 3}\n",
        "    top_violation = max(\n",
        "        violations,\n",
        "        key=lambda x: severity_rank[x[\"severity\"]]\n",
        "    )\n",
        "\n",
        "    decision = decision_rules[top_violation[\"severity\"]]\n",
        "    reasoning = (\n",
        "        f\"Content violates {top_violation['policy']} policy \"\n",
        "        f\"with {top_violation['severity']} severity.\"\n",
        "    )\n",
        "\n",
        "    return decision, reasoning\n"
      ],
      "metadata": {
        "id": "vWxOEaDWusKU"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_response(\n",
        "    user_text,\n",
        "    decision,\n",
        "    violations,\n",
        "    reasoning,\n",
        "):\n",
        "    \"\"\"\n",
        "    Returns structured, explainable moderation output.\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"input_text\": user_text,\n",
        "        \"decision\": decision,\n",
        "        \"violated_policies\": violations,\n",
        "        \"reasoning\": reasoning,\n",
        "        \"explainability\": {\n",
        "            \"policy_driven\": True,\n",
        "            \"model_used_as_signal\": True,\n",
        "            \"human_readable_reasoning\": True\n",
        "        }\n",
        "    }\n"
      ],
      "metadata": {
        "id": "K_jFp9HUuvJx"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def moderate_content(text):\n",
        "    # Step 1: Get model signals\n",
        "    model_signals = get_model_signals(text)\n",
        "\n",
        "    # Step 2: Extract policy violations\n",
        "    violations = extract_policy_violation(\n",
        "        model_signals,\n",
        "        MODERATION_POLICIES\n",
        "    )\n",
        "\n",
        "    # Step 3: Make final decision\n",
        "    decision, reasoning = make_moderation_decision(\n",
        "        violations,\n",
        "        DECISION_RULES\n",
        "    )\n",
        "\n",
        "    # Step 4: Build structured output\n",
        "    response = build_response(\n",
        "        text,\n",
        "        decision,\n",
        "        violations,\n",
        "        reasoning,\n",
        "    )\n",
        "\n",
        "    return response\n"
      ],
      "metadata": {
        "id": "eF9z8pOBu1yx"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I hate all of you and you deserve pain.\"\n",
        "raw_output = moderation_pipeline(text)\n",
        "print(\"Raw model output:\")\n",
        "print(raw_output)\n",
        "\n",
        "MODEL_TO_POLICY_MAP = {\n",
        "    \"H\": \"HATE_SPEECH\",\n",
        "    \"HR\": \"HARASSMENT\",\n",
        "    \"V\": \"VIOLENCE\",\n",
        "    \"SH\": \"SELF_HARM\",\n",
        "    \"S\": \"SEXUAL_CONTENT\",\n",
        "    \"S3\": \"SEXUAL_CONTENT\", # S3 usually indicates more severe sexual content, mapping to general sexual policy for now.\n",
        "    \"H2\": \"VIOLENCE\",        # H2 indicates hate speech with a threat, which aligns with the VIOLENCE policy.\n",
        "    \"V2\": \"VIOLENCE\",        # V2 indicates graphic violence, mapping to the VIOLENCE policy.\n",
        "}\n",
        "\n",
        "result_fixed = moderate_content(text)\n",
        "print(\"\\nModeration result with corrected policy mapping:\")\n",
        "print(result_fixed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pzjppb5u_SA",
        "outputId": "17917064-6422-430f-b6ca-cd879e1ad8b6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw model output:\n",
            "[[{'label': 'S', 'score': 0.046954259276390076}, {'label': 'H', 'score': 0.2959465980529785}, {'label': 'V', 'score': 0.23995475471019745}, {'label': 'HR', 'score': 0.21563826501369476}, {'label': 'SH', 'score': 0.19797274470329285}, {'label': 'S3', 'score': 0.032017480581998825}, {'label': 'H2', 'score': 0.0597618892788887}, {'label': 'V2', 'score': 0.04814678058028221}, {'label': 'OK', 'score': 0.17626331746578217}]]\n",
            "\n",
            "Moderation result with corrected policy mapping:\n",
            "{'input_text': 'I hate all of you and you deserve pain.', 'decision': 'DISALLOW', 'violated_policies': [{'policy': 'HATE_SPEECH', 'severity': 'MEDIUM', 'confidence': 0.296, 'evidence_label': 'H'}, {'policy': 'VIOLENCE', 'severity': 'HIGH', 'confidence': 0.24, 'evidence_label': 'V'}, {'policy': 'HARASSMENT', 'severity': 'MEDIUM', 'confidence': 0.216, 'evidence_label': 'HR'}, {'policy': 'SELF_HARM', 'severity': 'HIGH', 'confidence': 0.198, 'evidence_label': 'SH'}], 'reasoning': 'Content violates VIOLENCE policy with HIGH severity.', 'explainability': {'policy_driven': True, 'model_used_as_signal': True, 'human_readable_reasoning': True}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"I like you so much you are my favourite\"\n",
        "result = moderate_content(sample_text)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JYrI_N13bX5",
        "outputId": "94ca3dcc-4f80-4bd1-d551-b5b8316a0a70"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_text': 'I like you so much you are my favourite',\n",
              " 'decision': 'ALLOW',\n",
              " 'violated_policies': [],\n",
              " 'reasoning': 'No policy violations detected.',\n",
              " 'explainability': {'policy_driven': True,\n",
              "  'model_used_as_signal': True,\n",
              "  'human_readable_reasoning': True}}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"I hate your content. It gives me so much pain watching\"\n",
        "result = moderate_content(sample_text)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IzJymMD6QxU",
        "outputId": "ed2d7082-0824-4798-b30b-3997b0deb0d8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_text': 'I hate your content. It gives me so much pain watching',\n",
              " 'decision': 'DISALLOW',\n",
              " 'violated_policies': [{'policy': 'SEXUAL_CONTENT',\n",
              "   'severity': 'HIGH',\n",
              "   'confidence': 0.306,\n",
              "   'evidence_label': 'S'},\n",
              "  {'policy': 'HARASSMENT',\n",
              "   'severity': 'MEDIUM',\n",
              "   'confidence': 0.1,\n",
              "   'evidence_label': 'HR'},\n",
              "  {'policy': 'SELF_HARM',\n",
              "   'severity': 'HIGH',\n",
              "   'confidence': 0.16,\n",
              "   'evidence_label': 'SH'}],\n",
              " 'reasoning': 'Content violates SEXUAL_CONTENT policy with HIGH severity.',\n",
              " 'explainability': {'policy_driven': True,\n",
              "  'model_used_as_signal': True,\n",
              "  'human_readable_reasoning': True}}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"Wow thats ammazing\"\n",
        "result = moderate_content(sample_text)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbH_mhP89KS6",
        "outputId": "5b90f7ea-79c4-4e74-bb7d-21b20385abd6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_text': 'Wow thats ammazing',\n",
              " 'decision': 'ALLOW',\n",
              " 'violated_policies': [],\n",
              " 'reasoning': 'No policy violations detected.',\n",
              " 'explainability': {'policy_driven': True,\n",
              "  'model_used_as_signal': True,\n",
              "  'human_readable_reasoning': True}}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23c3fa74"
      },
      "source": [
        "You can use the built-in `input()` function in Python to get input directly from the user in a Colab code cell. When you run the cell, a text box will appear for the user to type their input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5bdd6e2",
        "outputId": "c848a096-d3e6-4cce-e2b4-13a562d6b91b"
      },
      "source": [
        "user_text_input = input(\"Please enter the text you want to moderate: \")\n",
        "\n",
        "# Now you can use 'user_text_input' with your moderation function\n",
        "moderation_result = moderate_content(user_text_input)\n",
        "print(\"\\nModeration Result:\")\n",
        "print(moderation_result)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter the text you want to moderate: i hate it\n",
            "\n",
            "Moderation Result:\n",
            "{'input_text': 'i hate it', 'decision': 'DISALLOW', 'violated_policies': [{'policy': 'HATE_SPEECH', 'severity': 'MEDIUM', 'confidence': 0.119, 'evidence_label': 'H'}, {'policy': 'HARASSMENT', 'severity': 'MEDIUM', 'confidence': 0.103, 'evidence_label': 'HR'}, {'policy': 'SELF_HARM', 'severity': 'HIGH', 'confidence': 0.111, 'evidence_label': 'SH'}], 'reasoning': 'Content violates SELF_HARM policy with HIGH severity.', 'explainability': {'policy_driven': True, 'model_used_as_signal': True, 'human_readable_reasoning': True}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def gradio_moderation_interface(text):\n",
        "    result = moderate_content(text)\n",
        "    return result\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=gradio_moderation_interface,\n",
        "    inputs=gr.Textbox(\n",
        "        lines=4,\n",
        "        placeholder=\"Enter text for moderation\"\n",
        "    ),\n",
        "    outputs=\"json\",\n",
        "    title=\"Policy-Driven LLM Content Moderation\",\n",
        "    description=\"LLM-assisted, policy-based moderation with explainable reasoning\"\n",
        ")\n",
        "\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "4bltifZt-sU0",
        "outputId": "21a8e367-6695-4ecf-8a9b-08e2894e82c3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://6f29c41d6f531066ef.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://6f29c41d6f531066ef.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61a2ce47",
        "outputId": "270081b7-27cf-49e1-c77d-5d6f70faa13e"
      },
      "source": [
        "# Import the Python SDK\n",
        "import google.generativeai as genai\n",
        "# Used to securely store your API key\n",
        "from google.colab import userdata\n",
        "\n",
        "# Configure the API key\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "print(\"Gemini API configured.\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemini API configured.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e401779",
        "outputId": "2086d366-4e6d-462f-eae6-901458af5390"
      },
      "source": [
        "# Initialize the Gemini GenerativeModel\n",
        "# Using 'gemini-pro' for general text generation\n",
        "gemini_reasoning_model = genai.GenerativeModel('gemini-pro')\n",
        "\n",
        "print(\"Gemini Generative Model initialized.\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemini Generative Model initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc0b0257"
      },
      "source": [
        "def moderate_content(text):\n",
        "    # Step 1: Get model signals\n",
        "    model_signals = get_model_signals(text)\n",
        "\n",
        "    # Step 2: Extract policy violations\n",
        "    violations = extract_policy_violation(\n",
        "        model_signals,\n",
        "        MODERATION_POLICIES\n",
        "    )\n",
        "\n",
        "    # Step 3: Make final decision\n",
        "    decision, reasoning = make_moderation_decision(\n",
        "        violations,\n",
        "        DECISION_RULES\n",
        "    )\n",
        "\n",
        "    # Step 4: Build structured output\n",
        "    response = build_response(\n",
        "        text,\n",
        "        decision,\n",
        "        violations,\n",
        "        reasoning,\n",
        "    )\n",
        "\n",
        "    # Step 5: Generate Gemini explanation if violations exist\n",
        "    if violations:\n",
        "        # Build a more comprehensive prompt for Gemini\n",
        "        detailed_reasoning_prompt = build_reasoning_prompt(\n",
        "            user_text=text,\n",
        "            model_scores=model_signals,\n",
        "            policies=MODERATION_POLICIES\n",
        "        )\n",
        "\n",
        "        # Add a final instruction for Gemini to explain the decision\n",
        "        detailed_reasoning_prompt += f\"\\n\\nBased on the above, explain the moderation decision '{decision}' for the user content, specifically mentioning any triggering words/phrases and relating them to the violated policies ({', '.join(v['policy'] for v in violations)}).\"\n",
        "\n",
        "        try:\n",
        "            gemini_response = gemini_reasoning_model.generate_content(detailed_reasoning_prompt)\n",
        "            response[\"gemini_explanation\"] = gemini_response.text\n",
        "        except Exception as e:\n",
        "            response[\"gemini_explanation\"] = f\"Error generating Gemini explanation: {e}\"\n",
        "    else:\n",
        "        response[\"gemini_explanation\"] = \"No specific reasoning generated as no policy violations were detected.\"\n",
        "\n",
        "    return response"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_gemini_explanation_prompt(moderation_result):\n",
        "    \"\"\"\n",
        "    Builds a safe, policy-faithful prompt for Gemini\n",
        "    to explain the moderation outcome.\n",
        "    \"\"\"\n",
        "\n",
        "    return f\"\"\"\n",
        "You are an AI assistant explaining a content moderation decision\n",
        "to a non-technical user.\n",
        "\n",
        "IMPORTANT RULES:\n",
        "- Do NOT change the decision.\n",
        "- Do NOT introduce new policies.\n",
        "- Do NOT add assumptions.\n",
        "- Explain ONLY based on the information provided.\n",
        "\n",
        "Moderation Decision:\n",
        "{moderation_result[\"decision\"]}\n",
        "\n",
        "Reasoning (policy-based):\n",
        "{moderation_result[\"reasoning\"]}\n",
        "\n",
        "Violations:\n",
        "{moderation_result[\"violated_policies\"]}\n",
        "\n",
        "Explain clearly and politely why this content was allowed or disallowed.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "eqZaezKpRGv2"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import time\n",
        "\n",
        "user_input = input(\"Please enter the text you want to moderate: \")\n",
        "\n",
        "moderation_result = moderate_content(user_input)\n",
        "\n",
        "print(\"\\n--- Detailed Moderation Result ---\")\n",
        "print(json.dumps(moderation_result, indent=2))\n",
        "\n",
        "print(\"\\n--- Gemini Explanation ---\")\n",
        "\n",
        "gemini_prompt = build_gemini_explanation_prompt(moderation_result)\n",
        "\n",
        "try:\n",
        "    print(\"Calling Gemini...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    response = gemini_reasoning_model.generate_content(gemini_prompt)\n",
        "\n",
        "    print(\"Gemini responded in\", round(time.time() - start_time, 2), \"seconds\\n\")\n",
        "    print(response.text)\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Gemini explanation unavailable.\")\n",
        "    print(\"Fallback explanation:\")\n",
        "    print(moderation_result[\"reasoning\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtl8DMEwR-mU",
        "outputId": "fc71b1a1-15e9-4e0e-dae0-bfe86c40ec46"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter the text you want to moderate: I hate sleping\n",
            "\n",
            "--- Detailed Moderation Result ---\n",
            "{\n",
            "  \"input_text\": \"I hate sleping\",\n",
            "  \"decision\": \"DISALLOW\",\n",
            "  \"violated_policies\": [\n",
            "    {\n",
            "      \"policy\": \"SEXUAL_CONTENT\",\n",
            "      \"severity\": \"HIGH\",\n",
            "      \"confidence\": 0.108,\n",
            "      \"evidence_label\": \"S\"\n",
            "    },\n",
            "    {\n",
            "      \"policy\": \"HATE_SPEECH\",\n",
            "      \"severity\": \"MEDIUM\",\n",
            "      \"confidence\": 0.26,\n",
            "      \"evidence_label\": \"H\"\n",
            "    },\n",
            "    {\n",
            "      \"policy\": \"VIOLENCE\",\n",
            "      \"severity\": \"HIGH\",\n",
            "      \"confidence\": 0.14,\n",
            "      \"evidence_label\": \"V\"\n",
            "    },\n",
            "    {\n",
            "      \"policy\": \"HARASSMENT\",\n",
            "      \"severity\": \"MEDIUM\",\n",
            "      \"confidence\": 0.227,\n",
            "      \"evidence_label\": \"HR\"\n",
            "    }\n",
            "  ],\n",
            "  \"reasoning\": \"Content violates SEXUAL_CONTENT policy with HIGH severity.\",\n",
            "  \"explainability\": {\n",
            "    \"policy_driven\": true,\n",
            "    \"model_used_as_signal\": true,\n",
            "    \"human_readable_reasoning\": true\n",
            "  },\n",
            "  \"gemini_explanation\": \"Error generating Gemini explanation: Invalid leading whitespace, reserved character(s), or return character(s) in header value: 'AIzaSyAMNV4i5xTR10mS-Ij04qZht1h27qx_Wyo\\\\r\\\\nAIzaSyAMNV4i5xTR10mS-Ij04qZht1h27qx_Wyo'\"\n",
            "}\n",
            "\n",
            "--- Gemini Explanation ---\n",
            "Calling Gemini...\n",
            "Gemini explanation unavailable.\n",
            "Fallback explanation:\n",
            "Content violates SEXUAL_CONTENT policy with HIGH severity.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Explanation templates for policy-based reasoning\n",
        "\n",
        "POSITIVE_EXPLANATION = (\n",
        "    \"The content was reviewed against all moderation policies and was found to be safe. \"\n",
        "    \"It does not contain hate speech, harassment, violence, self-harm references, \"\n",
        "    \"or any other restricted content.\"\n",
        ")\n",
        "\n",
        "POLICY_EXPLANATION_TEMPLATES = {\n",
        "    \"HATE_SPEECH\": (\n",
        "        \"The content contains hostile or demeaning language directed at a person or group, \"\n",
        "        \"which violates the Hate Speech policy.\"\n",
        "    ),\n",
        "    \"HARASSMENT\": (\n",
        "        \"The content includes abusive or aggressive language directed at an individual, \"\n",
        "        \"which falls under the Harassment policy.\"\n",
        "    ),\n",
        "    \"VIOLENCE\": (\n",
        "        \"The content expresses or promotes physical harm or threats, such as wishing pain \"\n",
        "        \"or injury, which violates the Violence policy.\"\n",
        "    ),\n",
        "    \"SELF_HARM\": (\n",
        "        \"The content references or encourages self-harm, which violates the Self-Harm policy.\"\n",
        "    ),\n",
        "    \"SEXUAL_CONTENT\": (\n",
        "        \"The content contains sexually explicit or inappropriate references that are \"\n",
        "        \"not suitable for general audiences.\"\n",
        "    )\n",
        "}\n"
      ],
      "metadata": {
        "id": "DDITPKo8SGRs"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_policy_explanation(decision, violations):\n",
        "    \"\"\"\n",
        "    Generates a human-readable explanation for both\n",
        "    allowed and disallowed content using policy templates.\n",
        "    \"\"\"\n",
        "\n",
        "    # Case 1: Content is allowed\n",
        "    if decision == \"ALLOW\":\n",
        "        return POSITIVE_EXPLANATION\n",
        "\n",
        "    # Case 2: Content is disallowed or restricted\n",
        "    explanations = []\n",
        "\n",
        "    for v in violations:\n",
        "        policy = v[\"policy\"]\n",
        "        if policy in POLICY_EXPLANATION_TEMPLATES:\n",
        "            explanations.append(POLICY_EXPLANATION_TEMPLATES[policy])\n",
        "\n",
        "    # Remove duplicates while preserving order\n",
        "    explanations = list(dict.fromkeys(explanations))\n",
        "\n",
        "    return \" \".join(explanations)\n"
      ],
      "metadata": {
        "id": "3xltRFlVSRZc"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def gradio_moderation_interface(text):\n",
        "    result = moderate_content(text)\n",
        "    return result\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=gradio_moderation_interface,\n",
        "    inputs=gr.Textbox(\n",
        "        lines=4,\n",
        "        placeholder=\"Enter text for moderation\"\n",
        "    ),\n",
        "    outputs=\"json\",\n",
        "    title=\"Policy-Driven LLM Content Moderation\",\n",
        "    description=\"LLM-assisted, policy-based moderation with explainable reasoning\"\n",
        ")\n",
        "\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "aLPUA8wOSSBs",
        "outputId": "26157094-8306-4232-e1f9-4955cf85554b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://f2b7de7bce2633730c.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f2b7de7bce2633730c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QfTSn7ZtSX0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "894b6e7e"
      },
      "source": [
        "import json\n",
        "\n",
        "def gradio_moderation_interface(text):\n",
        "    result = moderate_content(text)\n",
        "    full_result_json = json.dumps(result, indent=2)\n",
        "    gemini_explanation = result.get(\"gemini_explanation\", \"No explanation available.\")\n",
        "    return full_result_json, gemini_explanation\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f42eadd"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the `gradio_moderation_interface` function is updated to return two distinct outputs (JSON string and Gemini explanation), the Gradio `Interface` needs to be modified to handle these two outputs using `gr.JSON` and `gr.Textbox` components respectively.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "7ebdcd04",
        "outputId": "9c6b7099-7bed-45a6-cfe4-c3ef25f19c6c"
      },
      "source": [
        "import gradio as gr\n",
        "\n",
        "def gradio_moderation_interface(text):\n",
        "    result = moderate_content(text)\n",
        "    full_result_json = json.dumps(result, indent=2)\n",
        "    gemini_explanation = result.get(\"gemini_explanation\", \"No explanation available.\")\n",
        "    return full_result_json, gemini_explanation\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=gradio_moderation_interface,\n",
        "    inputs=gr.Textbox(\n",
        "        lines=4,\n",
        "        placeholder=\"Enter text for moderation\"\n",
        "    ),\n",
        "    outputs=[\n",
        "        gr.JSON(label=\"Full Moderation Result\"),\n",
        "        gr.Textbox(label=\"Gemini Explanation\", lines=10)\n",
        "    ],\n",
        "    title=\"Policy-Driven LLM Content Moderation\",\n",
        "    description=\"LLM-assisted, policy-based moderation with explainable reasoning\"\n",
        ")\n",
        "\n",
        "demo.launch(share=True)\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://74481bce78e9248363.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://74481bce78e9248363.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    }
  ]
}